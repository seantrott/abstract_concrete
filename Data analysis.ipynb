{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling similarity judgments with word embeddings\n",
    "\n",
    "Previous work has illustrated that similarity judgments between words can be estimated using distributional models of word meaning. But are there certain kinds of words for which distributioanl information is more or less useful?\n",
    "\n",
    "There are theoretical reasons to think that distributional information is more helpful for learning abstract concepts/words (Lupyan & Winter, 2018), and some empirical evidence as well (Kiros et al, 2018). But so far, others have not asked whether distributional information better predicts similarity judgments as a function of the *level of abstractness* of a word, or other kinds of semantic features. \n",
    "\n",
    "We use the Brysbaert norms as an estimate of concreteness, and ELMo as a model for word embeddings.\n",
    "\n",
    "For the pairwise similarity judgments, we will consider several different datasets in turn.\n",
    "\n",
    "We consider two approaches:\n",
    "\n",
    "## Approach 1\n",
    "\n",
    "Approach 1 is simple. For a given dataset, we classify each wordpair into one of three bins: `Abstract`, `Concrete`, and `Mixed`. `Abstract` corresponds to a wordpair in which both words fall below the median concreteness for that dataset, `Concrete` corresponds to a wordpair in which both words fall above, and `Mixed` means that one falls below and one falls above.\n",
    "\n",
    "We then regress `similarity ~ cosine_distance` for Abstract wordpairs only, and Concrete wordpairs only, and compare the resulting `R^2` values.\n",
    "\n",
    "### Median split \n",
    "\n",
    "Because each dataset isn't balanced in terms of its concreteness, we can't necessarily use the median split from the Brysbaert norms. Thus, we run and report two approaches to splitting the data:\n",
    "\n",
    "1) Using the median concreteness from a given dataset.  \n",
    "2) Using the median concreteness from the four datasets we consider.\n",
    "\n",
    "(1) will ensure that a given dataset is balanced, but might result in the same words being considered `Abstract` or `Concrete` in different datasets.\n",
    "\n",
    "(2) will result in less balanced datasets, but still more balanced than using the single Brysbaert norm, and will ensure consistency in which words are categorized as `Abstract` or `Concrete` across datasets.\n",
    "\n",
    "## Approach 2\n",
    "\n",
    "A median split of concreteness is useful for model interpretation, but is also problematic for two reasons:\n",
    "\n",
    "1. It dichotomizes a variable that may be better characterized as continuous.  \n",
    "2. Each dataset isn't balanced in terms of its concreteness; that is, the median concreteness in the Brysbaert norms isn't necessarily the same as the median concreteness in each similarity rating dataset.\n",
    "\n",
    "One solution is to ask whether the concreteness of each word in the word pairs predicts a given word pair's contribution to the fit of a model. That is, we can fit the entire dataset, then iteratively remove different wordpairs and fit the dataset after each removal. A given wordpair can thus be characterized in terms of whether its removal impairs or improves model fit. We can then ask whether a wordpair's contribution is related to each of those words' concreteness. \n",
    "\n",
    "The `contribution` of a wordpair is defined as `original_r2 - new_r2`, where `new_r2` is the fit of the model without that wordpair. Thus, a more positive value means that removing that wordpair **impairs** the model (the fit is lower without the wordpair), and a more negative value means that removing that wordpair **improves** the model (the fit is higher without the wordpair).\n",
    "\n",
    "Thus, a positive coefficient for `w1_concreteness`, `w2_concreteness`, or their interaction means that more concrete words (or wordpairs) improve the model, e.g., they have a *positive* contribution on R2. A negative coefficient means that more abstract words (or wordpairs) improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_type(row):\n",
    "    if row['w1_is_abstract'] and row['w2_is_abstract']:\n",
    "        return \"Abstract\"\n",
    "    elif not row['w1_is_abstract'] and not row['w2_is_abstract']:\n",
    "        return \"Concrete\"\n",
    "    return \"Mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_as_abstract(df, median_concreteness_amount):\n",
    "    \"\"\"Tag each word in a word pair as whether it is concrete, according to the median concreteness.\"\"\"\n",
    "    df['w1_is_abstract'] = df['w1_conc'].apply(lambda x: x < median_concreteness_amount)\n",
    "    df['w2_is_abstract'] = df['w2_conc'].apply(lambda x: x < median_concreteness_amount)\n",
    "    df['same'] = df['w1_is_abstract'] == df['w2_is_abstract']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(df):\n",
    "    \"\"\"Print out dataset distribution in terms of comparisons.\"\"\"\n",
    "    print(\"#Abstract: {n}\".format(n=len(df[df['Comparison Type']=='Abstract'])))\n",
    "    print(\"#Concrete: {n}\".format(n=len(df[df['Comparison Type']=='Concrete'])))\n",
    "    print(\"#Mixed: {n}\".format(n=len(df[df['Comparison Type']=='Mixed'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_for_dataset(df, formula):\n",
    "    \"\"\"Get R2 and coefficients for different subsets of dataset.\"\"\"\n",
    "    results = []\n",
    "    result = sm.ols(formula=FORMULA, \n",
    "                data=df).fit()\n",
    "    results.append({\n",
    "        'comparison': 'overall',\n",
    "        'r2': result.rsquared,\n",
    "        'coef': result.params['decontextualized_elmo_similarity'],\n",
    "        'n': len(df)\n",
    "    })\n",
    "    for i in ['Abstract', 'Concrete', 'Mixed']:\n",
    "        df_reduced = df[df['Comparison Type']==i]\n",
    "        result = sm.ols(formula=FORMULA, \n",
    "                    data=df_reduced).fit()\n",
    "        results.append({\n",
    "            'comparison': i,\n",
    "            'r2': result.rsquared,\n",
    "            'coef': result.params['decontextualized_elmo_similarity'],\n",
    "            'n': len(df_reduced)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMULA = 'similarity ~ decontextualized_elmo_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_pair_out(df, formula):\n",
    "    \n",
    "    result = sm.ols(formula=formula, \n",
    "                data=df).fit()\n",
    "    original_coef = result.params['decontextualized_elmo_similarity']\n",
    "    \n",
    "    differences = []\n",
    "    for pair_index in tqdm(range(len(df))):\n",
    "        df_copy = df.copy()\n",
    "        df_copy = df_copy.drop(pair_index)\n",
    "        \n",
    "        result = sm.ols(formula=formula, \n",
    "            data=df_copy).fit()\n",
    "        new_coef = result.params['decontextualized_elmo_similarity']\n",
    "        differences.append(original_coef - new_coef)\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "\n",
    "Load each processed dataset and print out descriptive statistics for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3487\n",
      "824\n"
     ]
    }
   ],
   "source": [
    "df_sim = pd.read_csv(\"data/processed/sim3500_with_cosine_distance.csv\")\n",
    "combined = list(df_sim['w1']) + list(df_sim['w2'])\n",
    "print(len(df_sim))\n",
    "print(len(set(combined)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "437\n"
     ]
    }
   ],
   "source": [
    "df_wordsim = pd.read_csv(\"data/processed/wordsim_with_cosine_distance.csv\")\n",
    "combined = list(df_wordsim['Word 1']) + list(df_wordsim['Word 2'])\n",
    "print(len(df_wordsim))\n",
    "print(len(set(combined)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1028\n"
     ]
    }
   ],
   "source": [
    "df_simlex = pd.read_csv(\"data/processed/simlex_with_cosine_distance.csv\")\n",
    "combined = list(df_simlex['word1']) + list(df_simlex['word2'])\n",
    "print(len(df_simlex))\n",
    "print(len(set(combined)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754\n",
      "1094\n"
     ]
    }
   ],
   "source": [
    "df_mturk = pd.read_csv(\"data/processed/mturk771_with_cosine_distance.csv\")\n",
    "combined = list(df_mturk['w1']) + list(df_mturk['w2'])\n",
    "print(len(df_mturk))\n",
    "print(len(set(combined)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics about concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.03"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = list(df_sim['w1_conc']) +  list(df_sim['w2_conc'])\n",
    "simverb_conc = statistics.median(combined)\n",
    "simverb_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = list(df_wordsim['w1_conc']) +  list(df_wordsim['w2_conc'])\n",
    "wordsim_conc = statistics.median(combined)\n",
    "wordsim_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.73"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = list(df_simlex['w1_conc']) +  list(df_simlex['w2_conc'])\n",
    "simlex_conc = statistics.median(combined)\n",
    "simlex_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = list(df_mturk['w1_conc']) +  list(df_mturk['w2_conc'])\n",
    "mturk_conc = statistics.median(combined)\n",
    "mturk_conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.835"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_conc = statistics.median([simverb_conc, wordsim_conc, simlex_conc, mturk_conc])\n",
    "median_conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimVerb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using median concreteness for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 1164\n",
      "#Concrete: 1179\n",
      "#Mixed: 1144\n"
     ]
    }
   ],
   "source": [
    "df_sim = tag_as_abstract(df_sim, simverb_conc)\n",
    "df_sim['Comparison Type'] = df_sim.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.137249</td>\n",
       "      <td>overall</td>\n",
       "      <td>3487</td>\n",
       "      <td>0.169454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.443910</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1164</td>\n",
       "      <td>0.168931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.368142</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>1179</td>\n",
       "      <td>0.104506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-12.454512</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>1144</td>\n",
       "      <td>0.221531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef comparison     n        r2\n",
       "0 -10.137249    overall  3487  0.169454\n",
       "1  -9.443910   Abstract  1164  0.168931\n",
       "2  -8.368142   Concrete  1179  0.104506\n",
       "3 -12.454512      Mixed  1144  0.221531"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_sim, FORMULA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using median concreteness across all four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 2223\n",
      "#Concrete: 399\n",
      "#Mixed: 865\n"
     ]
    }
   ],
   "source": [
    "df_sim = tag_as_abstract(df_sim, median_conc)\n",
    "df_sim['Comparison Type'] = df_sim.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.137249</td>\n",
       "      <td>overall</td>\n",
       "      <td>3487</td>\n",
       "      <td>0.169454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.244710</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2223</td>\n",
       "      <td>0.182513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.036077</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>399</td>\n",
       "      <td>0.024355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-12.434353</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>865</td>\n",
       "      <td>0.217490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coef comparison     n        r2\n",
       "0 -10.137249    overall  3487  0.169454\n",
       "1 -10.244710   Abstract  2223  0.182513\n",
       "2  -4.036077   Concrete   399  0.024355\n",
       "3 -12.434353      Mixed   865  0.217490"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_sim, FORMULA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using median concreteness for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 101\n",
      "#Concrete: 138\n",
      "#Mixed: 114\n"
     ]
    }
   ],
   "source": [
    "df_wordsim = tag_as_abstract(df_wordsim, wordsim_conc)\n",
    "df_wordsim['Comparison Type'] = df_wordsim.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_wordsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.312924</td>\n",
       "      <td>overall</td>\n",
       "      <td>353</td>\n",
       "      <td>0.292742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.498852</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>101</td>\n",
       "      <td>0.364580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.768396</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>138</td>\n",
       "      <td>0.249650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.554172</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>114</td>\n",
       "      <td>0.217952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -8.312924    overall  353  0.292742\n",
       "1 -8.498852   Abstract  101  0.364580\n",
       "2 -7.768396   Concrete  138  0.249650\n",
       "3 -9.554172      Mixed  114  0.217952"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_wordsim, FORMULA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using median concreteness across all four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 95\n",
      "#Concrete: 150\n",
      "#Mixed: 108\n"
     ]
    }
   ],
   "source": [
    "df_wordsim = tag_as_abstract(df_wordsim, median_conc)\n",
    "df_wordsim['Comparison Type'] = df_wordsim.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_wordsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.312924</td>\n",
       "      <td>overall</td>\n",
       "      <td>353</td>\n",
       "      <td>0.292742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.764007</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>95</td>\n",
       "      <td>0.395844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.549924</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>150</td>\n",
       "      <td>0.239757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.409771</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>108</td>\n",
       "      <td>0.213429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -8.312924    overall  353  0.292742\n",
       "1 -8.764007   Abstract   95  0.395844\n",
       "2 -7.549924   Concrete  150  0.239757\n",
       "3 -9.409771      Mixed  108  0.213429"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_wordsim, FORMULA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimLex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using median concreteness for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 433\n",
      "#Concrete: 434\n",
      "#Mixed: 132\n"
     ]
    }
   ],
   "source": [
    "df_simlex = tag_as_abstract(df_simlex, simlex_conc)\n",
    "df_simlex['Comparison Type'] = df_simlex.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_simlex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.734998</td>\n",
       "      <td>overall</td>\n",
       "      <td>999</td>\n",
       "      <td>0.166691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.841296</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>433</td>\n",
       "      <td>0.169374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.649626</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>434</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.468034</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>132</td>\n",
       "      <td>0.253343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -7.734998    overall  999  0.166691\n",
       "1 -8.841296   Abstract  433  0.169374\n",
       "2 -6.649626   Concrete  434  0.157500\n",
       "3 -9.468034      Mixed  132  0.253343"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_simlex, FORMULA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using median concreteness across all four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 451\n",
      "#Concrete: 427\n",
      "#Mixed: 121\n"
     ]
    }
   ],
   "source": [
    "df_simlex = tag_as_abstract(df_simlex, median_conc)\n",
    "df_simlex['Comparison Type'] = df_simlex.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_simlex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.734998</td>\n",
       "      <td>overall</td>\n",
       "      <td>999</td>\n",
       "      <td>0.166691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.676706</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>451</td>\n",
       "      <td>0.166289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.611504</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>427</td>\n",
       "      <td>0.154087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.601434</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>121</td>\n",
       "      <td>0.256230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -7.734998    overall  999  0.166691\n",
       "1 -8.676706   Abstract  451  0.166289\n",
       "2 -6.611504   Concrete  427  0.154087\n",
       "3 -9.601434      Mixed  121  0.256230"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_simlex, FORMULA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTurk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Using median concreteness for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 244\n",
      "#Concrete: 246\n",
      "#Mixed: 264\n"
     ]
    }
   ],
   "source": [
    "df_mturk = tag_as_abstract(df_mturk, mturk_conc)\n",
    "df_mturk['Comparison Type'] = df_mturk.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_mturk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.549750</td>\n",
       "      <td>overall</td>\n",
       "      <td>754</td>\n",
       "      <td>0.326633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.012256</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>244</td>\n",
       "      <td>0.387926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.142269</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>246</td>\n",
       "      <td>0.272679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.865899</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>264</td>\n",
       "      <td>0.322219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -4.549750    overall  754  0.326633\n",
       "1 -5.012256   Abstract  244  0.387926\n",
       "2 -4.142269   Concrete  246  0.272679\n",
       "3 -4.865899      Mixed  264  0.322219"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_mturk, FORMULA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Using median concreteness across all four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract: 195\n",
      "#Concrete: 322\n",
      "#Mixed: 237\n"
     ]
    }
   ],
   "source": [
    "df_mturk = tag_as_abstract(df_mturk, median_conc)\n",
    "df_mturk['Comparison Type'] = df_mturk.apply(lambda row: get_comparison_type(row), axis=1)\n",
    "print_distribution(df_mturk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>comparison</th>\n",
       "      <th>n</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.549750</td>\n",
       "      <td>overall</td>\n",
       "      <td>754</td>\n",
       "      <td>0.326633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.746439</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>195</td>\n",
       "      <td>0.380477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.954131</td>\n",
       "      <td>Concrete</td>\n",
       "      <td>322</td>\n",
       "      <td>0.262926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.568677</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>237</td>\n",
       "      <td>0.345884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef comparison    n        r2\n",
       "0 -4.549750    overall  754  0.326633\n",
       "1 -4.746439   Abstract  195  0.380477\n",
       "2 -3.954131   Concrete  322  0.262926\n",
       "3 -5.568677      Mixed  237  0.345884"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_for_dataset(df_mturk, FORMULA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
